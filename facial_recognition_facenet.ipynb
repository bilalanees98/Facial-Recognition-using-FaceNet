{"cells":[{"metadata":{},"cell_type":"markdown","source":"# DM - project (based on facenet paper)\nby:\ni170093 - Shayaan Farooq\n\ni170305 - Muhammad Huzaifa\n\ni170317 - Muhammad Bilal\n\nMethod:\n\nWe have trained a 3-path siamese network, with offline triplet mining.\nThe classifier we have utilised is KNN\nThe model utilises the same network (zfnet) explained in the paper\nSpecifications are as follows:\n* LFW dataset, where 10 random classes (with 20+ images) are split off for test data\n* the images are centre cropped to reduce their size to 160x160\n* epochs = 10\n* margin = 0.2"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torchvision\nfrom torch.utils.data import  Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport os\nfrom skimage import io\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom itertools import combinations,permutations\n# from scipy.spatial.distance import cdist\nfrom scipy.spatial import distance\nfrom sklearn.metrics.pairwise import pairwise_distances\nimport time\nimport math\n# from google.colab import files\n# from torchvision.models import resnet34\nimport torchvision.models as models\nfrom PIL import Image\n# from torchvision.models import resnet152\nimport random","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class KNearestNeighbor(object):\n  \"\"\" a kNN classifier with L2 distance \"\"\"\n\n  def __init__(self):\n    pass\n\n  def train(self, X, y):\n    self.X_train = np.asarray(X)\n    self.y_train = np.asarray(y)\n    \n  def predict(self, X, k=1, num_loops=0):\n    if num_loops == 0:\n      dists = self.compute_distances_no_loops(X)\n    elif num_loops == 1:\n      dists = self.compute_distances_one_loop(X)\n    elif num_loops == 2:\n      dists = self.compute_distances_two_loops(X)\n    else:\n      raise ValueError('Invalid value %d for num_loops' % num_loops)\n\n    return self.predict_labels(dists, k=k)\n\n  def compute_distances_two_loops(self, X):\n    num_test = X.shape[0]\n    num_train = self.X_train.shape[0]\n    dists = np.zeros((num_test, num_train))\n    for i in range(num_test):\n      for j in range(num_train):\n        dists[i][j] = np.sqrt(np.sum((X[i] - self.X_train[j]) ** 2))\n    return dists\n\n  def compute_distances_one_loop(self, X):\n    num_test = X.shape[0]\n    num_train = self.X_train.shape[0]\n    dists = np.zeros((num_test, num_train))\n    for i in range(num_test):\n      dists[i] = np.sqrt(np.sum((self.X_train - X[i]) ** 2, axis=1))\n    return dists\n\n  def compute_distances_no_loops(self, X):\n    num_test = X.shape[0]\n    num_train = self.X_train.shape[0]\n    dists = np.zeros((num_test, num_train)) \n    s1 = np.sum(X ** 2, axis=1)\n    s2 = np.sum(self.X_train ** 2, axis=1)\n    s = s1.reshape((num_test, 1)) + s2 - 2 * X.dot(self.X_train.T)\n    dists = np.sqrt(s)\n    return dists\n\n  def predict_labels(self, dists, k=1):\n    num_test = dists.shape[0]\n    y_pred = np.zeros(num_test)\n    for i in range(num_test):\n      # A list of length k storing the labels of the k nearest neighbors to\n      # the ith test point.\n      closest_y = []\n\n      knn_ix = dists[i].argsort()[:k]\n      closest_y = self.y_train[knn_ix]\n\n      values, counts = np.unique(closest_y, return_counts=True)\n      y_pred[i] = values[counts == counts.max()].min()\n\n    return y_pred\n","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class testnet(nn.Module):\n    def __init__(self, embedding_size, pretrained=False):\n        super (testnet, self).__init__()\n        self.conv1 = nn.Conv2d(3,64, 7, stride=2)\n        self.pool1 = nn.MaxPool2d(3, stride=2, padding = 1)\n        self.rnorm1 = nn.BatchNorm2d(64)\n        self.relu1 = nn.ReLU()\n\n        self.conv2a = nn.Conv2d(64,64, 1, stride=1)\n        self.conv2 = nn.Conv2d(64,192, 3, stride=1)\n        self.rnorm2 = nn.BatchNorm2d(192)\n        self.pool2 = nn.MaxPool2d(3, stride=2, padding = 1)\n        self.relu2 = nn.ReLU()\n\n        self.conv3a = nn.Conv2d(192,192, 1, stride=1)\n        self.conv3 = nn.Conv2d(192,384, 3, stride=1)\n        self.pool3 = nn.MaxPool2d(3, stride=2,padding = 1)\n        self.relu3 = nn.ReLU()\n\n        self.conv4a = nn.Conv2d(384,384, 1, stride=1)\n        self.conv4 = nn.Conv2d(384,256, 3, stride=1, padding = 1)\n\n        self.conv5a = nn.Conv2d(256,256, 1, stride=1)\n        self.conv5 = nn.Conv2d(256,256, 3, stride=1, padding = 1)\n\n        self.conv6a = nn.Conv2d(256,256, 1, stride=1)\n        self.conv6 = nn.Conv2d(256,256, 3, stride=1, padding = 1)\n\n        self.pool4 = nn.MaxPool2d(3, stride=2, padding = 1)\n        #concatinating\n#         2304\n        self.fc1 = nn.Linear(6400, 128*32*1)#what is maxout\n        self.fc2 = nn.Linear(128*32*1, 128*32*1)#what is maxout\n        self.fc7128 = nn.Linear(128*32*1, 128*1*1)#what is maxout\n        #l2 normalization\n\n    def l2_norm(self, input):\n        input_size = input.size()\n        buffer     = torch.pow(input, 2)\n        normp      = torch.sum(buffer, 1).add_(1e-10)\n        norm       = torch.sqrt(normp)\n        _output    = torch.div(input, norm.view(-1, 1).expand_as(input))\n        output     = _output.view(input_size)\n\n        return output\n\n    def forward_once(self, x):\n        x = self.conv1(x)\n        x = self.pool1(x)\n        x = self.rnorm1(x)\n        x = self.relu1(x)\n\n        x = self.conv2a(x)\n        x = self.conv2(x)\n        x = self.rnorm2(x)\n        x = self.pool2(x)\n        x = self.relu2(x)\n\n        x = self.conv3a(x)\n        x = self.conv3(x)\n        x = self.pool3(x)\n        x = self.relu3(x)\n\n        x = self.conv4a(x)\n        x = self.conv4(x)\n        x = self.conv5a(x)\n        x = self.conv5(x)\n        x = self.conv6a(x)\n        x = self.conv6(x)\n        x = self.pool4(x)\n\n        #concatinating\n        x = x.view(x.size(0), -1)\n\n        x = self.fc1(x)\n        x = self.fc2(x)\n        x = self.fc7128(x)\n        #l2 normalization\n        x = self.l2_norm(x)\n        return x\n\n    def forward(self, anchor, positive, negative):\n#         for triplet\n        a = self.forward_once(anchor)\n        p = self.forward_once(positive)\n        n = self.forward_once(negative)\n\n        return a, p, n\n\nclass OfflineTripletLoss(nn.Module):\n    def __init__(self, margin):\n        super(OfflineTripletLoss, self).__init__()\n        self.margin = margin\n    def forward(self, a, p , n):\n#         do something here to improve convergance, maybe\n# check distances for hard and semi hard triplets, turn easy triplet distance to maybe 0?\n        distance_positive = (a - p).pow(2).sum(1)\n        distance_negative = (a - n).pow(2).sum(1)\n        temp = distance_positive - distance_negative + self.margin\n#         print (temp)\n        losses = F.relu(temp)\n        return losses.mean()","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#LFW dataset inheriting 'Dataset'\nclass LFWDataset(Dataset):\n    \n    \n    def __init__(self, csv_file, root_dir, testFoldersRequired, imageThreshold, transform):\n        #we gotta data load\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.annotations = pd.read_csv(csv_file) #contains csv read data    \n        self.root_dir = root_dir\n        self.transform = transform\n        self.testFoldersRequired=testFoldersRequired,\n        self.imageThreshold= imageThreshold,\n        self.trainImagePaths=[] #contains path that get_item uses to return image tensor\n        self.trainImageLabels=[]; #corresponding to above paths, this contains labels, also returned in get_item\n        \n        self.testImagePaths=[] #retrived using getTestImagePaths function and used in other custom dataset for testing\n        self.testImageLabels= [] #corresponding labels\n\n        self.triplets= []\n        self.testFolders=[] #folder numbers correspond to rows in csv\n        self.testFolderCount=int(0)\n        self.annotationLength= len(self.annotations)\n        self.trainFolders=[] #all other folders than test\n        \n        \n        #class separator\n        #random so that each time we train/separate classes its different\n        for i in random.sample(range(self.annotationLength), self.annotationLength):\n                totalImagesForLabel= int(self.annotations.iloc[i, 1])\n                \n                #if testfolders are less than 10, and images in current folder are more than 10, SEPARATE IT\n                if(self.testFolderCount < self.testFoldersRequired[0] and totalImagesForLabel >self.imageThreshold[0]): #[0] is done cuz its tuple idk why but it works\n                    self.testFolders.append(i) #adds csv file row number to testFolder\n                    self.testFolderCount+=1\n                else:\n                    self.trainFolders.append(i) # add folder to training \n        \n        #main loop, loops each person in train Folders\n        for i in self.trainFolders:\n            \n            totalImagesForLabel= int(self.annotations.iloc[i, 1])\n            #this loop generates path for each image for a person\n            #if condition decides if it needs to be added to trainset or testset\n            for j in range(totalImagesForLabel):  \n                #All are sent to train\n                self.trainImageLabels.append(i)\n                self.trainImagePaths.append(root_dir \n                                     + \"/\" \n                                     + self.annotations.iloc[i, 0] \n                                     + \"/\" \n                                     + self.annotations.iloc[i, 0] \n                                     + '_' \n                                     + \"%0004d\" % (j+1) + \".jpg\")\n\n                #generating triplet\n                anchor=len(self.trainImageLabels) - 1 #index in trainImages\n                    \n                if(totalImagesForLabel<5):\n                    self.generateTriplet(anchor,j, totalImagesForLabel) #j is position in folder\n                    \n                elif(totalImagesForLabel<10):\n                    self.generateTriplet(anchor,j, totalImagesForLabel)\n                    self.generateTriplet(anchor,j, totalImagesForLabel)\n                    \n                elif(totalImagesForLabel<15):\n                    self.generateTriplet(anchor,j, totalImagesForLabel)\n                    self.generateTriplet(anchor,j, totalImagesForLabel)\n                    self.generateTriplet(anchor,j, totalImagesForLabel)\n                    \n                elif(totalImagesForLabel<20):\n                    self.generateTriplet(anchor,j, totalImagesForLabel)\n                    self.generateTriplet(anchor,j, totalImagesForLabel)\n                    self.generateTriplet(anchor,j, totalImagesForLabel)\n                    self.generateTriplet(anchor,j, totalImagesForLabel)\n                    \n                else:\n                    self.generateTriplet(anchor,j, totalImagesForLabel)\n                    self.generateTriplet(anchor,j, totalImagesForLabel)\n                    self.generateTriplet(anchor,j, totalImagesForLabel)\n                    self.generateTriplet(anchor,j, totalImagesForLabel)\n                    self.generateTriplet(anchor,j, totalImagesForLabel)\n\n                    \n                    \n                \n                \n        #UNCOMMENT TO SEE SPLIT NUMBERS WITH CURRENT CONFIGS IN if elif elifs ABOVE\n        print(\"No of Train Images: \" + str(len(self.trainImagePaths)))\n        print(\"No of Triplets: \" + str(len(self.triplets)))\n        \n    def __len__(self):\n        #does this: len(dataset for training)\n        return len(self.triplets)\n\n    #this will return image, label from the train set\n    def __getitem__(self, idx):\n        triplet= self.triplets[idx]\n        \n        anc_img_path= self.trainImagePaths[triplet[0]]\n        anc_image=Image.open(anc_img_path)\n#         anc_image= anc_image.crop((73, 91, 173, 191))\n        anc_image= anc_image.crop((43, 61, 203, 221))\n\n        label_for_image= self.trainImageLabels[triplet[0]] # number eg. 15th row in csv file \n            \n        pos_img_path= self.trainImagePaths[triplet[1]]\n        pos_image=Image.open(pos_img_path)\n        pos_image= pos_image.crop((43, 61, 203, 221))\n        \n        neg_img_path= self.trainImagePaths[triplet[2]]\n        neg_image=Image.open(neg_img_path)\n        neg_image= neg_image.crop((43, 61, 203, 221))\n        \n        if self.transform:\n            anc_image = self.transform(anc_image)\n            pos_image = self.transform(pos_image)\n            neg_image = self.transform(neg_image)\n\n        return (anc_image, pos_image, neg_image, label_for_image)\n\n    def generateTriplet(self, anchor, j, totalImagesForLabel):\n        triplet=[]\n        triplet.append(anchor) #anchor appended\n        triplet.append(self.generatePositive(anchor, j , totalImagesForLabel)) #j is achors number in folder\n        triplet.append(self.generateNegative(anchor-j, totalImagesForLabel))\n        self.triplets.append(triplet)\n            \n        \n    \n    def generatePositive(self,a,positionInFolder, noOfImages):\n        if(noOfImages==1):\n            return a\n        \n        rangeForRandom= list(range(0,positionInFolder)) + list(range(positionInFolder+1, noOfImages))\n        randomNo=random.choice(rangeForRandom)\n        if(randomNo>positionInFolder):\n            return a+(randomNo - positionInFolder) #position that its going to have in imagePathArray\n        else:\n            return a-(positionInFolder - randomNo)\n        \n    def generateNegative(self, startIndexOfLabel, noOfImagesForLabel):\n        rangeForRandom= list(range(0,startIndexOfLabel)) + list(range(startIndexOfLabel+noOfImagesForLabel+1, 12817))\n        return random.choice(rangeForRandom)\n        \n    def getTriplets(self):\n        return self.triplets\n    \n    def getTestFolders(self):\n        return self.testFolders\n    \n    def getTrainImagePaths(self):\n        return self.trainImagePaths\n        \n    def getTrainImageLabels(self):\n        return self.trainImageLabels\n    \n    def getTestImagePaths(self):\n        return self.testImagePaths\n\n    def getTestImageLabels(self):\n        return self.testImageLabels\n    \n    def getNameForLabel(self,i):\n        return self.annotations.iloc[i,0] #gets 0, returns AJ_Cook or whatever first persons name is innit\n","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class separatedClassesDataset(Dataset):\n    \n    \n    def __init__(self, csv_file, root_dir, testFolders, noOfImagesToKeepForTestPerFolder, transform):\n        #we gotta data load\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            testImageP(list of strings): contains paths to images separated for training\n            testImageL(list of ints): contains labels to above images\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.annotations = pd.read_csv(csv_file) #contains csv read data\n        self.root_dir = root_dir\n        self.transform = transform\n        \n        self.testFolders=testFolders\n        self.noOfImagesToKeepForTestPerFolder= noOfImagesToKeepForTestPerFolder \n\n        #this will contain paths to those images jinki embedddings niklayn gi for knn\n        self.forwardPassImagePaths=[]\n        self.forwardPassImageLabels=[]\n        \n        #this will contain paths to images that we gotta test after embedding+knn\n        self.testImagePaths=[] \n        self.testImageLabels=[]\n        self.noOfImagesToForwardPerFolder = 0\n        \n        for i in testFolders:\n            numAddedToForward=0\n            totalImagesForLabel= int(self.annotations.iloc[i, 1])\n            \n            self.noOfImagesToForwardPerFolder = totalImagesForLabel - self.noOfImagesToKeepForTestPerFolder\n            \n            for j in random.sample(range(totalImagesForLabel), totalImagesForLabel):\n                    if(numAddedToForward<self.noOfImagesToForwardPerFolder):\n                        self.forwardPassImageLabels.append(i)\n                        self.forwardPassImagePaths.append(root_dir \n                                         + \"/\" \n                                         + self.annotations.iloc[i, 0] \n                                         + \"/\" \n                                         + self.annotations.iloc[i, 0] \n                                         + '_' \n                                         + \"%0004d\" % (j+1) + \".jpg\")\n                        \n                        numAddedToForward+=1\n                        \n                    else:\n                        self.testImageLabels.append(i)\n                        self.testImagePaths.append(root_dir \n                                         + \"/\" \n                                         + self.annotations.iloc[i, 0] \n                                         + \"/\" \n                                         + self.annotations.iloc[i, 0] \n                                         + '_' \n                                         + \"%0004d\" % (j+1) + \".jpg\")\n                        \n            \n        \n    def __len__(self):\n        \n        return len(self.forwardPassImagePaths)\n\n    #this will return image, label\n    def __getitem__(self, idx):\n        img_path= self.forwardPassImagePaths[idx]\n        image=Image.open(img_path)\n        image= image.crop((43, 61, 203, 221))\n#         (73, 91, 173, 191)\n        label_for_image= self.forwardPassImageLabels[idx]\n        \n        if self.transform:\n            image = self.transform(image)\n\n        return (image, label_for_image)\n\n    def getTestImagePaths(self):\n        return self.testImagePaths\n    \n    def getTestImageLabels(self):\n        return self.testImageLabels\n    \n    def getNameForLabel(self,i):\n        return self.annotations.iloc[i,0] #gets 0, returns AJ_Cook or whatever first persons name is innit\n","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LFWTestDataset(Dataset):\n    \n    \n    def __init__(self, csv_file, root_dir, testImagePaths, testImageLabels, transform):\n        #we gotta data load\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            testImageP(list of strings): contains paths to images separated for training\n            testImageL(list of ints): contains labels to above images\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.annotations = pd.read_csv(csv_file) #contains csv read data\n        self.root_dir = root_dir\n        self.transform = transform\n        \n        self.testImagePaths=testImagePaths\n        self.testImageLabels= testImageLabels \n        \n        \n        \n        \n    def __len__(self):\n        #does this: len(dataset for testing)\n        return len(self.testImagePaths)\n\n    #this will return image, label from the test set\n    def __getitem__(self, idx):\n        img_path= self.testImagePaths[idx]\n        image=Image.open(img_path)\n        #print(\"precrop size: \")\n        #print(image.size)\n        image= image.crop((43, 61, 203, 221))\n#         (73, 91, 173, 191)\n        #print(\"postcrop size: \")\n        #print(image.size)\n        label_for_image= self.testImageLabels[idx] # number eg. 15th row in csv file \n\n        if self.transform:\n            image = self.transform(image)\n\n        return (image, label_for_image)\n\n    def getNameForLabel(self,i):\n        return self.annotations.iloc[i,0] #gets 0, returns AJ_Cook or whatever first persons name is innit\n","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LFWTrainDataset(Dataset):\n    \n    \n    def __init__(self, csv_file, root_dir, trainImagePaths, trainImageLabels, transform):\n        #we gotta data load\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            testImageP(list of strings): contains paths to images separated for training\n            testImageL(list of ints): contains labels to above images\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.annotations = pd.read_csv(csv_file) #contains csv read data\n        self.root_dir = root_dir\n        self.transform = transform\n        \n        self.trainImagePaths=trainImagePaths\n        self.trainImageLabels= trainImageLabels \n\n        \n    def __len__(self):\n        #does this: len(dataset for testing)\n        return len(self.trainImagePaths)\n\n    #this will return image, label from the test set\n    def __getitem__(self, idx):\n        img_path= self.trainImagePaths[idx]\n        image=Image.open(img_path)\n        #print(\"precrop size: \")\n        #print(image.size)\n#         image= image.crop((73, 91, 173, 191))\n        image= image.crop((43, 61, 203, 221))\n#         print(\"postcrop size: \")\n#         print(image.size)\n        label_for_image= self.trainImageLabels[idx] # number eg. 15th row in csv file \n\n        if self.transform:\n            image = self.transform(image)\n\n        return (image, label_for_image)\n\n    def getNameForLabel(self,i):\n        return self.annotations.iloc[i,0] #gets 0, returns AJ_Cook or whatever first persons name is innit\n","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Set Paths\nroot= '/kaggle/input/lfw-dataset/lfw-deepfunneled/lfw-deepfunneled'\ncsvFile= '/kaggle/input/lfw-dataset/lfw_allnames.csv'\n\n\n#==================for training this dawg=================================  \ntrainDataset= LFWDataset(csv_file= csvFile, \n                      root_dir= root,\n                      testFoldersRequired=10 ,\n                      imageThreshold=20 ,\n                      transform= transforms.ToTensor())\n\nprint(\"Test Data Classes: \")\nprint(trainDataset.annotations.iloc[trainDataset.testFolders])\n\n\n\n\n#singlke image train dataset FOR KNNNNNNN\nsingleImageTrainDataset = LFWTrainDataset(csv_file= csvFile, \n                      root_dir= root,\n                      trainImagePaths= trainDataset.getTrainImagePaths(),\n                      trainImageLabels= trainDataset.getTrainImageLabels(),      \n                      transform= transforms.ToTensor())\n\n\n\n#==================for embeddings then knn=================================  \ntestFolders=trainDataset.getTestFolders()\nforwardPassDataset = separatedClassesDataset(root_dir= root,\n                                             csv_file= csvFile,\n                                             testFolders=testFolders,\n                                             noOfImagesToKeepForTestPerFolder= 10,\n                                             transform= transforms.ToTensor()\n                                             ) \n# print(\"\\n-----------Forward Pass Paths------------------------\")\n# for i in range(len(forwardPassDataset.forwardPassImagePaths)):\n#     print(forwardPassDataset.forwardPassImagePaths[i])\n\n\n\n#==================for prediction=================================\ntestImagePaths=forwardPassDataset.getTestImagePaths() #[rock1.jpg, rock2.jpg, AJ_Cook.jpg]\ntestImageLabels=forwardPassDataset.getTestImageLabels() #[233, 233, 0] -> corresponds to csv/ folder numbers\n\ntestDataset = LFWTestDataset(root_dir= root,\n                             csv_file= csvFile,\n                             testImagePaths=testImagePaths,\n                             testImageLabels= testImageLabels,\n                             transform= transforms.ToTensor()\n                             ) \n\n# print(\"\\n-----------Test Paths-----------------\")\n# for i in range(len(testDataset.testImagePaths)):\n#     print(testDataset.testImagePaths[i])\n\n","execution_count":31,"outputs":[{"output_type":"stream","text":"No of Train Images: 12854\nNo of Triplets: 28265\nTest Data Classes: \n                   name  images\n223     Amelie_Mauresmo      21\n4333       Pete_Sampras      22\n4836   Rudolph_Giuliani      26\n1892  Gerhard_Schroeder     109\n786        Carlos_Menem      21\n3434      Mahmoud_Abbas      29\n373        Ariel_Sharon      77\n1207      David_Beckham      31\n2514     Jennifer_Lopez      21\n3327  Lindsay_Davenport      22\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tripletTrainLoader= torch.utils.data.DataLoader(trainDataset, batch_size=32, shuffle=True, num_workers=2)\nsingleTrainLoader= torch.utils.data.DataLoader(singleImageTrainDataset, batch_size=32, shuffle=True, num_workers=2)\nforwardTestLoader= torch.utils.data.DataLoader(forwardPassDataset, batch_size=32, shuffle=True, num_workers=2)\nsingleTestLoader= torch.utils.data.DataLoader(testDataset, batch_size=32, shuffle=True, num_workers=2)","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_EPOCHS = 10\nmargin = 0.2","execution_count":33,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Running this training cell just top show output of loss, actually trained/saved model is loaded in next cell "},{"metadata":{"trusted":true},"cell_type":"code","source":"# ---------------------training model for offline triplets/siamese network------------------------------\nmodel = testnet(128, False)\nmodel = model.cuda()\n\n# optimizer = optim.Adam(model.parameters(), lr=1e-4)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\nloss_func = OfflineTripletLoss(margin)\nloss_func = loss_func.cuda()\n\n\nfor epoch in range(NUM_EPOCHS):\n    start_time = time.time()\n    for i,data in enumerate(tripletTrainLoader):#this loader returns triplets from train dataset\n# will get 4 returns values instead of 2\n        anchors, positives, negatives, labels = data\n    \n        anchors = anchors.cuda()\n        positives = positives.cuda()\n        negatives = negatives.cuda()\n        # labels = labels.cuda()\n\n        aVectors,pVectors,nVectors = model(anchors, positives, negatives)\n        optimizer.zero_grad()\n\n# loss function does not take in labels\n        loss = loss_func(aVectors, pVectors , nVectors)\n        loss.backward()\n        optimizer.step()\n    print(\"--- %s seconds ---\" % (time.time() - start_time))\n    print('Epoch:{}, Loss:{}'.format(epoch+1, float(loss)))\n","execution_count":34,"outputs":[{"output_type":"stream","text":"--- 236.28417682647705 seconds ---\nEpoch:1, Loss:0.059837330132722855\n--- 227.43655848503113 seconds ---\nEpoch:2, Loss:0.057087793946266174\n--- 228.86970448493958 seconds ---\nEpoch:3, Loss:0.05640449747443199\n--- 237.65939712524414 seconds ---\nEpoch:4, Loss:0.03484893590211868\n--- 239.0373694896698 seconds ---\nEpoch:5, Loss:0.012365564703941345\n--- 236.2841441631317 seconds ---\nEpoch:6, Loss:0.0\n--- 237.3864140510559 seconds ---\nEpoch:7, Loss:0.0\n--- 233.12666273117065 seconds ---\nEpoch:8, Loss:0.0\n--- 233.63143372535706 seconds ---\nEpoch:9, Loss:0.0\n--- 240.45462608337402 seconds ---\nEpoch:10, Loss:0.0\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loading saved model parameters into a variable\nstate_dict = torch.load('/kaggle/input/trained-models/zfnet_10epoch_offline_32batch_160imsize.pth')\n# print(state_dict.keys())\n\n# updating model parameters with loaded variable\nmodel = testnet(128,  False)\nmodel.load_state_dict(state_dict)\nmodel = model.cuda()\n","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------creating training dps for final knn train---------------\nfinal_embeddings = torch.empty(0).float()\nfinal_labels = torch.empty(0)\nfor i, data in enumerate(singleTrainLoader):#this loader return single train images\n#     start_time = time.time()\n    imgs, labels = data\n    imgs = imgs.cuda()\n    \n    vectors = model.forward_once(imgs)\n    \n    final_embeddings = torch.cat((final_embeddings,vectors.detach().cpu()),0)\n    final_labels = torch.cat((final_labels,labels.float()),0)\n    \n#     print(\"--- %s seconds ---\" % (time.time() - start_time))\nprint (final_embeddings.size())","execution_count":35,"outputs":[{"output_type":"stream","text":"torch.Size([12854, 128])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-----------------creating forwarding dps from test data for knn training--------------\nforward_embeddings = torch.empty(0).float()\nforward_labels = torch.empty(0)\nfor i, data in enumerate(forwardTestLoader):#this loader return single test images\n#     start_time = time.time()\n    forwardimgs, forwardlabels = data\n    forwardimgs = forwardimgs.cuda()\n    \n    forwardvectors = model.forward_once(forwardimgs)\n    \n    forward_embeddings = torch.cat((forward_embeddings,forwardvectors.detach().cpu()),0)\n    forward_labels = torch.cat((forward_labels,forwardlabels.float()),0)\n    \n#     print(\"--- %s seconds ---\" % (time.time() - start_time))\nprint (forward_embeddings.size())","execution_count":37,"outputs":[{"output_type":"stream","text":"torch.Size([279, 128])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-----------------creating testing dps for knn classification--------------\ntest_embeddings = torch.empty(0).float()\ntest_labels = torch.empty(0)\nfor i, data in enumerate(singleTestLoader):#this loader return single test images\n#     start_time = time.time()\n    testimgs, testlabels = data\n    testimgs = testimgs.cuda()\n    \n    testvectors = model.forward_once(testimgs)\n    \n    test_embeddings = torch.cat((test_embeddings,testvectors.detach().cpu()),0)\n    test_labels = torch.cat((test_labels,testlabels.float()),0)\n    \n#     print(\"--- %s seconds ---\" % (time.time() - start_time))\nprint (test_embeddings.size())","execution_count":38,"outputs":[{"output_type":"stream","text":"torch.Size([100, 128])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train accuracy\n\nknn = KNearestNeighbor()\n\nknn.train(final_embeddings.numpy(),final_labels.numpy())\nans = knn.predict(final_embeddings.detach().numpy(),1,0)\n# print (ans[10:30])\nprint (ans)\nprint (final_labels)\ncount = 0.0\nfor i in range(len(final_labels)):\n    if(ans[i] == final_labels[i]):\n        count += 1\nprint (\"accuracy is: \" + str(count/len(final_labels)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # combining existing embeddings and forward embeddings\n# X = torch.cat((final_embeddings,forward_embeddings),0)\n# Y = torch.cat((final_labels,forward_labels),0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# revised test accuracy\nknn = KNearestNeighbor()\n\nknn.train(forward_embeddings.detach().numpy(),forward_labels.detach().numpy())\nans = knn.predict(test_embeddings.detach().numpy(),3,0)\n# ans = knn.predict(final_embeddings.detach().numpy(),1,0)\n# print (ans[10:30])\nprint (ans)\nprint (test_labels)\ncount = 0.0\nfor i in range(len(ans)):\n    if(ans[i] == test_labels[i]):\n        count += 1\nprint (\"accuracy is: \" + str(count/len(test_labels)))","execution_count":39,"outputs":[{"output_type":"stream","text":"[ 786. 4836.  223. 4836.  373.  223.  223. 1892. 4836. 1892.  223. 3327.\n  786. 2514. 1207. 2514. 4836. 3434.  786.  373. 1892.  223.  373. 4333.\n 1892.  373. 2514. 1207. 4333. 3327. 1207. 3327. 1892. 2514.  373.  223.\n 3434. 1892. 3434. 3434. 3434. 4836. 3327.  373.  223.  786. 4836. 3327.\n 1892.  373.  786. 1207. 4333.  223. 4333. 4333. 3434. 3434.  786. 3327.\n 4836. 4836. 1207. 1892. 3327. 1892. 1207.  786. 1207.  786.  786. 1207.\n 4836.  786. 2514. 2514. 2514. 1207. 4333.  786.  373.  223. 3434.  223.\n 2514. 4333.  373. 2514. 3434. 2514. 4333. 1892.  786.  373. 1892. 1207.\n 4333. 3327. 1207. 1207.]\ntensor([ 786., 4836.,  223., 4836.,  373.,  223., 3327., 1892., 4836., 1892.,\n         223., 3327.,  786., 2514., 1207., 2514., 4836., 3434.,  786.,  373.,\n        1892.,  223.,  373., 4333., 1892.,  373., 2514., 1207., 4333., 3327.,\n        3327., 3327., 1892., 2514.,  373.,  223., 3434., 1892., 3434., 3434.,\n        3434., 4836., 3327.,  373.,  223.,  786., 4836., 3327., 1892.,  373.,\n         786., 1207., 4333.,  223., 4333., 4333., 3434., 3434., 4333., 3327.,\n        4836., 4836., 1207., 1892., 3327., 1892., 3327.,  786., 1207.,  786.,\n         786., 1207., 4836.,  786., 2514., 2514., 2514., 1207., 4333.,  786.,\n         373.,  223., 3434.,  223., 2514., 4333.,  373., 2514., 3434., 2514.,\n        4333., 4836., 3434.,  373., 1892., 1207., 4333., 1207.,  223., 1207.])\naccuracy is: 0.92\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# saving the mdoel and downloading\ntorch.save(model.state_dict(), 'zfnet_10epoch_offline_500batch_160imsize.pth')\n\n# download checkpoint file\n# files.download('embeddingnet_5epoch_bathchard.pth')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}